{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing CNNs: Extras\n",
    "\n",
    "Unfortunately it is not so easy to visualize filters from deeper layers because they are not in the input space and they depend on the filters preceeding them in the network. We can do something to see them, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print X_train.shape\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Load trained model- change paths as appropriate\n",
    "model = model_from_json(open('../mnist_cnn.json').read())\n",
    "model.load_weights('../mnist_cnn_weights.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the input for visualization\n",
    "\n",
    "When we feed an input to out model it produces activations in the feature maps. We want to understand what a particular filter deep in the network is \"looking for\". We can do this by **optimizing the input** to make this feature as strongly activated as possible. We will look for an input image that most strongly activates a give filter.\n",
    "\n",
    "We can do this with gradient descent: rather than using gradient descent to change the weights of the network, we instead use gradient descent to change the input itself while keep the weights fixed. To do this, first we choose a filter in the network we wish to visualize and then define a loss function to encourage the filter to be strongly activated.\n",
    "\n",
    "Below is a helper function to do this. Given a model, the layer index, and the filter index in that layer, it will return a function that evaluates a loss and the gradients for the input to maximize that loss (we are doing gradient ascent in this case). This loss can be a variety of things, but here it is the mean activation of the filter over all the locations in the image - the higher this value is, the more the input activates the filter. In this case we are using Keras's backend (Theano or Tensorflow) directly to compute these gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_filter_iterator(model, layer, filter_ind):\n",
    "    input_img = model.layers[0].input\n",
    "    layer_output = model.layers[layer].output\n",
    "    \n",
    "    rank = len(model.layers[layer].output_shape)\n",
    "    \n",
    "    if rank == 4:\n",
    "        # This is a convolutional layer, so average over locations\n",
    "        loss = K.mean(layer_output[:, :, :, filter_ind])\n",
    "    elif rank == 2:\n",
    "        # This is a fully-connected layer\n",
    "        loss = K.mean(layer_output[:, filter_ind])\n",
    "\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5) # normalize the gradients (this is a trick that helps)\n",
    "    return K.function([input_img, K.learning_phase()], [loss, grads])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we select the layer and filter. Then we generate a random image as a starting point and do 20 steps of gradient ascent to maximize the activation of the selected filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = build_filter_iterator(model, 3, 0) # layer 3, filter 0\n",
    "\n",
    "input_img_data = np.random.random((1, 28, 28, 1))\n",
    "step = 0.1\n",
    "for i in range(20):\n",
    "    loss_value, grads_value = loss_fn([input_img_data, 0])\n",
    "    input_img_data += grads_value * step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can postprocess the image and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "x = deprocess_image(input_img_data[0])\n",
    "plt.imshow(np.squeeze(x), cmap='Greys', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Exercise 1 - interpretation\n",
    "\n",
    "1. Experiment with different filters and different layers spatial layers (layers that are not fully-connected).\n",
    "2. Try running the optimization for the same layer and filter multiple times - each run uses a different random starting point and could be different.\n",
    "3. Many times the optimized image will looks like it has a repeating pattern. Can you explain why the procedure we are using would create images with repetition?\n",
    "4. Compare some filters visualized in this way with the filter's feature maps. Can you see any correspondences?\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Exercise 2  - optimize for deeper layers\n",
    "\n",
    "We can visualize fully-connected layers deeper in the network with this technique as well. The `build_filter_iterator` method works on fully-connected layers as well. The only difference is that the loss function in this case does not average over spatial locations.\n",
    "\n",
    "It is particularly interesting to try the layer right before the softmax layer. Each dimension in the layer is interpretable and corresponds to a class label.\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maximize activation for deep layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Exercise 3 - different loss\n",
    "\n",
    "You can change the loss function defined inside `build_filter_iterator` to be anything you can imagine. Can you think of a different loss function to try? For example, you can try to maximize the activation of one unit while minimizing the activation of another. Or maximize one convolutional filter in a spatial region while minimizing another convolutional filter in another spatial region. The possibilities are endless.\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a `build_filter_iterator2` function and use it to experiment with different forms for the loss function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
