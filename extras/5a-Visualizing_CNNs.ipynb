{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing CNNs\n",
    "\n",
    "In this notebook we explore some common ways to visualize the operation of CNNs. These techniques are both valuable tools to debug and improve models as well as ways to understand what CNNs learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print X_train.shape\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the model we trained beforehand. This is done in two parts: we load the model structure from JSON and we load the model parameters with HDF5. The code below loads the model and parameters, compiles the model, and runs the model on the MNIST test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = model_from_json(open('../mnist_cnn.json').read())\n",
    "model.load_weights('../mnist_cnn_weights.h5')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing feature maps\n",
    "\n",
    "The the most basic way to understand what a deep model has learned is to look at the representations of the input in it's hidden layers. We did this before with MLPs and we can do it with CNNs too. With CNNs it is even more interesting because we can look at the hidden layer activations (\"feature maps\" in the common terminology of CNNs) directly as images.\n",
    "\n",
    "The first \"layer\" we can visualized is the input itself. This snippet chooses a random training example and displays it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = X_train[np.random.randint(len(X_train))]\n",
    "plt.imshow(np.squeeze(img), cmap='Greys') # the squeeze is needed to remove the channel dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can create a function to extract hidden layer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "get_layer1 = K.function([model.layers[0].input, K.learning_phase()], [model.layers[1].output])\n",
    "\n",
    "H = get_layer1([img[np.newaxis, :], 0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_layer1` will take an input image and return the feature maps for all filters in the first convolutional layer. When we use it, we need to make sure that all of the dimensions match. The model input has the shape `[batch, width, height, channels]`. We will be feeding one image at a time and we need to make the shape match the expected input. Note that we feed in a list of values: the first value is the input data and the second is a flag to tell Keras that we are not using the network in learning mode. This is important if the model as any dropout layers or other layers whose behavior is different during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print img.shape\n",
    "print img[np.newaxis, :].shape # This adds the batch dimension (size of 1 for 1 image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed in an image and check the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H = get_layer1([img[np.newaxis, :], 0])[0]\n",
    "print H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Exercise 1 - Layer shapes\n",
    "\n",
    "Based on your model and the size of the convolutional layers, explain why the output shape has the size it does. Check you explanation with the instructor or a classmate.\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the feature maps as images. For example, the line below will show the 10th feature map from the extracted features of the image we chose. Lighter areas correspond to areas where the feature is more \"on\" - i.e. where the corresponding filter matches the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(H[0, :, :, 10], cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at other filters by changing the `10` above. But let's make things easier and build a function to display _all_ the filters in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def show_all_feature_maps(H):\n",
    "    # make a square grid\n",
    "    num_maps = H.shape[1]\n",
    "    rows = int(np.ceil(np.sqrt(float(num_maps))))\n",
    "\n",
    "    fig = plt.figure(1, [10, 10])\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=[rows, rows])\n",
    "\n",
    "    for i in range(num_maps):\n",
    "        grid[i].axis('off')\n",
    "        grid[i].imshow(H[0, :, :, i], cmap='Greys_r')\n",
    "        \n",
    "    # Turn any unused axes off\n",
    "    for j in range(i, len(grid)):\n",
    "        grid[j].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_all_feature_maps(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Exercise 2 - Characterizing filters\n",
    "\n",
    "Using the handy visualization tool we just build, let's inspect what our model learned. Feed some examples in and visualize the feature maps.\n",
    "\n",
    "1. Can you find features that look like edge detectors?\n",
    "2. Try to find a filter that likes edges with a dark part on the left and a light part on the right.\n",
    "3. Try to find filters that like edges in different orientations.\n",
    "4. Can you find a filter that likes more than one edge orientation?\n",
    "\n",
    "Record which filters you find for the above questions and any others you find interesting.\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = X_train[np.random.randint(len(X_train))]\n",
    "plt.imshow(np.squeeze(img), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_all_feature_maps(get_layer1([img[np.newaxis, :]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Exercise 3 - visualizing the next conv layer\n",
    "\n",
    "Let's take a look a the next conv layer feature maps.\n",
    "\n",
    "1. Define a `get_layer2` function like `get_layer1` above that will extract the next convolutional layer output. It can be useful to print out `model.layers` to see what the index of the layer you want is.\n",
    "2. Are the feature maps more complex than the previous layer? If you think so, characterize how as best you can. Talk this over.\n",
    "3. Can you find filters that seem to \"like\" a particular number better than others? In what sense is this the case?\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyze the next conv layer by visualizing its feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing filters\n",
    "\n",
    "Another way to understand what a network has learned is to visualize it's filters. For the filters in the first layer, this can be done easily because those filters are in the space of the input image. This means we can display them as an image to get an idea of what they are trying to match. We can extract the weights from the layers in out model as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Conv weights:', model.layers[0].get_weights()[0].shape\n",
    "print 'Biases:', model.layers[0].get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some of the filters randomly by running the code below a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = model.layers[0].get_weights()[0] # get the conv weights as W\n",
    "F = np.squeeze(W)[:, :, np.random.randint(len(W))] # just choose a random weight\n",
    "\n",
    "plt.imshow(F, cmap='Greys_r', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Exercise 4 - Edge detector filters\n",
    "\n",
    "In the feature maps we already saw you might have spotted filters that seem to \"like\" edges. Plot some of the filters below and compare them to the corresponding feature map. Explain with some examples that you find how these edge detectors work. \n",
    "\n",
    "Remember: light corresponds you large weights and dark corresponds to low weights. It is a good idea to print out the filters weight themselves with `print F` to make sure this checks out.\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find and plot edge detector filters\n",
    "F = np.squeeze(W)[:, :, 0]\n",
    "plt.imshow(F, cmap='Greys_r', interpolation='none')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
