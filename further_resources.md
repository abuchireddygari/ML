
There are lot's of ways to continue learning deep learning. The best
way is to keep you hands dirty: keep playing around with data, try
tweaking existing model code online, apply your skills to you own
problem, and read about what other people are doing. Below is a list
of resources organized by topic to help you find the right launching
off point for your interests.


Basic Concepts
--------------

The Neural Network Playground is a great reference for the basic decisions
that you can control when creating a model. Playing with it can improve your
intuitions.

    http://playground.tensorflow.org/


This deep learning glossary is also a good reference for common
terminology, with links to more information in many cases.

    http://www.wildml.com/deep-learning-glossary/


This blog from Chris Olah of Google Brain has many fantastic, well-illustrated
articles introducing some of the fundamental concepts of deep learning in a
friendly way.

    http://colah.github.io/


In particular, this article is a great explanation of how neural networks
can learn to distort space to make their task easier.

    http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/



Computer Vision
---------------

Stanford CS231n lecture notes on convolution are a great review of what we
talked about in the course.

    http://cs231n.github.io/convolutional-networks/


The entire course is fantastic. The early lecture notes are a good review of
the basic concepts we covered. The full course provides a great comprehensive
introduction of deep learning for computer vision.

    http://cs231n.github.io/


The Stanford CS231n course videos can be downloaded here:

    https://www.dropbox.com/sh/7dw76x8ok6lk9ao/AACdnnvJrZJaQnRuteEy4JqBa?lst



Visualization
-------------

More information and examples of tSNE can be found here:

    https://lvdmaaten.github.io/tsne/


A comparison of dimensionality reduction techniques for
MNIST along with nice explanations:

    http://colah.github.io/posts/2014-10-Visualizing-MNIST/



Recurrent models
----------------

Andrej Karpathy's blog post on recurrent networks is both a great
introduction as wel as a fun demonstration of their capabilities.

    http://karpathy.github.io/2015/05/21/rnn-effectiveness/


This blog post introduces LSTMs in a very approachable way with
nice illustration and discussion.

    http://colah.github.io/posts/2015-08-Understanding-LSTMs/


The Magenta project is a public project to apply deep learning for
the creation of new musical compositions. Their first public models
are recurrent models for MIDI sequences.

    https://github.com/tensorflow/magenta/


Tensorflow
----------

The Udacity course by Google is a nice introduction for Tensorflow
with exercises. It is a great way to continue practicing course
concepts and delve deeper into Tensorflow.

    https://www.udacity.com/course/deep-learning--ud730


Keras
-----

The Keras blog demonstrates some advanced uses of Keras along with
information about the development of Keras.

    http://blog.keras.io/


The Keras examples are a good reference when creating models in Keras.

    https://github.com/fchollet/keras/tree/master/examples


Advanced
--------

The Deep Learning textbook in preparation for publication is free to read
online. It is a comprehense source of detailed and advanced information about
deep learning models and algorthms. It is an authoratative overview of the
field.

    http://www.deeplearningbook.org/


NIPS is a large machine learning conference. Many of the workshop talks from
2015 are available online and provide a view of what current research topics
are.

    https://nips.cc/Conferences/2015/Schedule?type=Workshop

